{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "For a quick start, we compare the different algorithms for deconvolution on the famous IRIS data set, estimating the distribution of Iris plant types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the example data\n",
    "using MLDataUtils\n",
    "X, y_labels, _ = load_iris()\n",
    "\n",
    "# discretize the target quantity (for numerical values, we'd use LinearDiscretizer)\n",
    "using Discretizers: encode, CategoricalDiscretizer\n",
    "y = encode(CategoricalDiscretizer(y_labels), y_labels) # vector of target value indices\n",
    "\n",
    "# have a look at the content of y\n",
    "unique(y) # its just indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into training and observed data sets.\n",
    "# \n",
    "# The matrices MLDataUtils expects are transposed, by default.\n",
    "# Thus, we have to be explicit about obsdim = 1. Note that\n",
    "# CherenkovDeconvolution.jl follows the convention of ScikitLearn.jl\n",
    "# (and others), which is size(X_train) == (n_examples, n_features).\n",
    "# \n",
    "# MLDataUtils unfortunately assumes size(X_train) == (n_features, n_examples),\n",
    "# but obsdim = 1 fixes this assumption.\n",
    "# \n",
    "using Random\n",
    "Random.seed!(42) # make split reproducible\n",
    "(X_train, y_train), (X_data, y_data) = splitobs(shuffleobs((X', y), obsdim = 1), obsdim = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution with DSEA\n",
    "\n",
    "The Dortmund Spectrum Estimation Algorithm (DSEA) reconstructs the target distribution from classifier predictions on the target quantity of individual examples. CherenkovDeconvolution.jl implements the improved version DSEA+, which is extended by adaptive step sizes and a fixed reweighting of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: DSEA iteration 1/1 uses alpha = 1.0 (chi2s = 0.0028011676660929376)\n",
      "└ @ CherenkovDeconvolution /home/mbunse/.julia/dev/CherenkovDeconvolution/src/methods/dsea.jl:119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3333333327844613\n",
       " 0.3549289670574495\n",
       " 0.3117377001580892"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn, CherenkovDeconvolution\n",
    "\n",
    "# deconvolve with a Naive Bayes classifier\n",
    "@sk_import naive_bayes : GaussianNB\n",
    "tp_function = Sklearn.train_and_predict_proba(GaussianNB()) # trains and applies the classifier in each iteration\n",
    "                                                            # Sklearn is a sub-module of CherenkovDeconvolution.jl\n",
    "\n",
    "f_dsea = dsea(X_data, X_train, y_train, tp_function) # returns a vector of target value probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3333333333333333 \n",
       " 0.35555555555555557\n",
       " 0.3111111111111111 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the result to the true target distribution, which we are estimating\n",
    "f_true = Util.fit_pdf(y_data) # f_dsea is almost equal to f_true!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Unfolding and Iterative Bayesian Unfolding\n",
    "\n",
    "RUN fits the target distribution `f` to the convolution model `g = R * f`, using maximum likelihood. The regularization strength is configured with `n_df`, the effective number of degrees of freedom in the second-order local model of the solution.\n",
    "\n",
    "IBU reconstructs the target distribution by iteratively applying Bayes' rule to the conditional probabilities contained in the detector response matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# RUN and IBU are only applicable with a single discrete observable dimension. In order to\n",
    "# obtain a dimension that contains as much information as possible, we discretize the feature\n",
    "# space with a decision tree, using its leaves as clusters. The cluster indices are the\n",
    "# discrete values of the observed dimension. This concepts relates to supervised clustering.\n",
    "#\n",
    "td = Sklearn.TreeDiscretizer(X_train, y_train, 6) # obtain (up to) 6 clusters\n",
    "x_train = encode(td, X_train)\n",
    "x_data  = encode(td, X_data)\n",
    "\n",
    "# have a look at the content of x_train\n",
    "unique(x_train) # its the cluster indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Limiting RUN to 3 of 6 observeable non-zero bins\n",
      "└ @ CherenkovDeconvolution /home/mbunse/.julia/dev/CherenkovDeconvolution/src/methods/run.jl:82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.32062181199902845\n",
       " 0.34272528540199176\n",
       " 0.33665290259897984"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, RUN and IBU do not need a classifier for deconvolution\n",
    "f_run = CherenkovDeconvolution.run(x_data, x_train, y_train) # module qualification required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3333333333333333\n",
       " 0.3463872738499534\n",
       " 0.3202793928167133"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_ibu = ibu(x_data, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22m f_\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22m Gri\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22mrch \u001b[0m\u001b[1mD\u001b[22men\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mA\u001b[22mrray \u001b[0m\u001b[1mD\u001b[22men\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mM\u001b[0m\u001b[1ma\u001b[22mtrix \u001b[0m\u001b[1mD\u001b[22men\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mVecOrM\u001b[0m\u001b[1ma\u001b[22mt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "dsea(data, train, y, train_and_predict_proba[, bins;\n",
       "     features = setdiff(names(train), [y]),\n",
       "     kwargs...])\n",
       "```\n",
       "\n",
       "Deconvolve the `y` distribution in the DataFrame `data`, as learned from the DataFrame `train`. This function wraps `dsea(::Matrix, ::Matrix, ::Array, ::Function)`.\n",
       "\n",
       "The additional keyword argument allows to specify the columns in `data` and `train` to be used as the `features`.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "dsea(X_data, X_train, y_train, train_and_predict_proba[, bins; kwargs...])\n",
       "```\n",
       "\n",
       "Deconvolve the target distribution of `X_data`, as learned from `X_train` and `y_train`.\n",
       "\n",
       "The function `train_and_predict_proba(X_data, X_train, y_train, w_train) -> Any` trains and applies a classifier. All of its arguments but `w_train`, which is updated in each iteration, are simply passed through from `dsea`. To facilitate classification, `y_train` has to be discrete, i.e., it must contain label indices rather than actual values. All expected indices (for cases where `y_train` may not contain some of the indices) are optionally provided as `bins`.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `f_0 = ones(m) ./ m` defines the prior, which is uniform by default\n",
       "  * `fixweighting = true` sets, whether or not the weight update fix is applied. This fix is proposed in my Master's thesis and in the corresponding paper.\n",
       "  * `alpha = 1.0` is the step size taken in every iteration. This parameter can be either a constant value or a function with the signature `(k::Int, pk::AbstractArray{Float64,1}, f_prev::AbstractArray{Float64,1} -> Float`, where `f_prev` is the estimate of the previous iteration and `pk` is the direction that DSEA takes in the current iteration `k`.\n",
       "  * `smoothing = Base.identity` is a function that optionally applies smoothing in between iterations\n",
       "  * `K = 1` is the maximum number of iterations.\n",
       "  * `epsilon = 0.0` is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "  * `inspect = nothing` is a function `(f_k::Array, k::Int, chi2s::Float64, alpha::Float64) -> Any` optionally called in every iteration.\n",
       "  * `return_contributions = false` sets, whether or not the contributions of individual examples in `X_data` are returned as a tuple together with the deconvolution result.\n"
      ],
      "text/plain": [
       "\u001b[36m  dsea(data, train, y, train_and_predict_proba[, bins;\u001b[39m\n",
       "\u001b[36m       features = setdiff(names(train), [y]),\u001b[39m\n",
       "\u001b[36m       kwargs...])\u001b[39m\n",
       "\n",
       "  Deconvolve the \u001b[36my\u001b[39m distribution in the DataFrame \u001b[36mdata\u001b[39m, as learned from the\n",
       "  DataFrame \u001b[36mtrain\u001b[39m. This function wraps \u001b[36mdsea(::Matrix, ::Matrix, ::Array,\n",
       "  ::Function)\u001b[39m.\n",
       "\n",
       "  The additional keyword argument allows to specify the columns in \u001b[36mdata\u001b[39m and\n",
       "  \u001b[36mtrain\u001b[39m to be used as the \u001b[36mfeatures\u001b[39m.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  dsea(X_data, X_train, y_train, train_and_predict_proba[, bins; kwargs...])\u001b[39m\n",
       "\n",
       "  Deconvolve the target distribution of \u001b[36mX_data\u001b[39m, as learned from \u001b[36mX_train\u001b[39m and\n",
       "  \u001b[36my_train\u001b[39m.\n",
       "\n",
       "  The function \u001b[36mtrain_and_predict_proba(X_data, X_train, y_train, w_train) ->\n",
       "  Any\u001b[39m trains and applies a classifier. All of its arguments but \u001b[36mw_train\u001b[39m, which\n",
       "  is updated in each iteration, are simply passed through from \u001b[36mdsea\u001b[39m. To\n",
       "  facilitate classification, \u001b[36my_train\u001b[39m has to be discrete, i.e., it must contain\n",
       "  label indices rather than actual values. All expected indices (for cases\n",
       "  where \u001b[36my_train\u001b[39m may not contain some of the indices) are optionally provided\n",
       "  as \u001b[36mbins\u001b[39m.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36mf_0 = ones(m) ./ m\u001b[39m defines the prior, which is uniform by default\n",
       "\n",
       "    •    \u001b[36mfixweighting = true\u001b[39m sets, whether or not the weight update fix is\n",
       "        applied. This fix is proposed in my Master's thesis and in the\n",
       "        corresponding paper.\n",
       "\n",
       "    •    \u001b[36malpha = 1.0\u001b[39m is the step size taken in every iteration. This\n",
       "        parameter can be either a constant value or a function with the\n",
       "        signature \u001b[36m(k::Int, pk::AbstractArray{Float64,1},\n",
       "        f_prev::AbstractArray{Float64,1} -> Float\u001b[39m, where \u001b[36mf_prev\u001b[39m is the\n",
       "        estimate of the previous iteration and \u001b[36mpk\u001b[39m is the direction that\n",
       "        DSEA takes in the current iteration \u001b[36mk\u001b[39m.\n",
       "\n",
       "    •    \u001b[36msmoothing = Base.identity\u001b[39m is a function that optionally applies\n",
       "        smoothing in between iterations\n",
       "\n",
       "    •    \u001b[36mK = 1\u001b[39m is the maximum number of iterations.\n",
       "\n",
       "    •    \u001b[36mepsilon = 0.0\u001b[39m is the minimum symmetric Chi Square distance between\n",
       "        iterations. If the actual distance is below this threshold,\n",
       "        convergence is assumed and the algorithm stops.\n",
       "\n",
       "    •    \u001b[36minspect = nothing\u001b[39m is a function \u001b[36m(f_k::Array, k::Int,\n",
       "        chi2s::Float64, alpha::Float64) -> Any\u001b[39m optionally called in every\n",
       "        iteration.\n",
       "\n",
       "    •    \u001b[36mreturn_contributions = false\u001b[39m sets, whether or not the\n",
       "        contributions of individual examples in \u001b[36mX_data\u001b[39m are returned as a\n",
       "        tuple together with the deconvolution result."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?dsea # You can find more information in the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "run(data, train, y, x; kwargs...)\n",
       "```\n",
       "\n",
       "Regularized Unfolding of the target distribution in the DataFrame `data`. The deconvolution is inferred from the DataFrame `train`, where the target column `y` and the observable column `x` are given.\n",
       "\n",
       "This function wraps `run(R, g; kwargs...)`, constructing `R` and `g` from the examples in the two DataFrames.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "run(x_data, x_train, y_train; kwargs...)\n",
       "```\n",
       "\n",
       "Regularized Unfolding of the target distribution, given the observations in the one-dimensional array `x_data`. The deconvolution is inferred from `x_train` and `y_train`.\n",
       "\n",
       "This function wraps `run(R, g; kwargs...)`, constructing `R` and `g` from the examples in the three arrays.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "run(R, g; kwargs...)\n",
       "```\n",
       "\n",
       "Perform RUN with the observed frequency distribution `g` (absolute counts!) and the detector response matrix `R`.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `n_df = size(R, 2)` is the effective number of degrees of freedom. The default `n_df` results in no regularization (there is one degree of freedom for each dimension in the result).\n",
       "  * `K = 100` is the maximum number of iterations.\n",
       "  * `epsilon = 1e-6` is the minimum difference in the loss function between iterations. RUN stops when the absolute loss difference drops below `epsilon`.\n",
       "  * `inspect = nothing` is a function `(f_k::Array, k::Int, ldiff::Float64, tau::Float64) -> Any` optionally called in every iteration.\n"
      ],
      "text/plain": [
       "\u001b[36m  run(data, train, y, x; kwargs...)\u001b[39m\n",
       "\n",
       "  Regularized Unfolding of the target distribution in the DataFrame \u001b[36mdata\u001b[39m. The\n",
       "  deconvolution is inferred from the DataFrame \u001b[36mtrain\u001b[39m, where the target column\n",
       "  \u001b[36my\u001b[39m and the observable column \u001b[36mx\u001b[39m are given.\n",
       "\n",
       "  This function wraps \u001b[36mrun(R, g; kwargs...)\u001b[39m, constructing \u001b[36mR\u001b[39m and \u001b[36mg\u001b[39m from the\n",
       "  examples in the two DataFrames.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  run(x_data, x_train, y_train; kwargs...)\u001b[39m\n",
       "\n",
       "  Regularized Unfolding of the target distribution, given the observations in\n",
       "  the one-dimensional array \u001b[36mx_data\u001b[39m. The deconvolution is inferred from \u001b[36mx_train\u001b[39m\n",
       "  and \u001b[36my_train\u001b[39m.\n",
       "\n",
       "  This function wraps \u001b[36mrun(R, g; kwargs...)\u001b[39m, constructing \u001b[36mR\u001b[39m and \u001b[36mg\u001b[39m from the\n",
       "  examples in the three arrays.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  run(R, g; kwargs...)\u001b[39m\n",
       "\n",
       "  Perform RUN with the observed frequency distribution \u001b[36mg\u001b[39m (absolute counts!)\n",
       "  and the detector response matrix \u001b[36mR\u001b[39m.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36mn_df = size(R, 2)\u001b[39m is the effective number of degrees of freedom.\n",
       "        The default \u001b[36mn_df\u001b[39m results in no regularization (there is one degree\n",
       "        of freedom for each dimension in the result).\n",
       "\n",
       "    •    \u001b[36mK = 100\u001b[39m is the maximum number of iterations.\n",
       "\n",
       "    •    \u001b[36mepsilon = 1e-6\u001b[39m is the minimum difference in the loss function\n",
       "        between iterations. RUN stops when the absolute loss difference\n",
       "        drops below \u001b[36mepsilon\u001b[39m.\n",
       "\n",
       "    •    \u001b[36minspect = nothing\u001b[39m is a function \u001b[36m(f_k::Array, k::Int,\n",
       "        ldiff::Float64, tau::Float64) -> Any\u001b[39m optionally called in every\n",
       "        iteration."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?CherenkovDeconvolution.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1mu\u001b[22m f_\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1mu\u001b[22m \u001b[0m\u001b[1mI\u001b[22mO\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1mu\u001b[22mffer @\u001b[0m\u001b[1mi\u001b[22mn\u001b[0m\u001b[1mb\u001b[22mo\u001b[0m\u001b[1mu\u001b[22mnds P\u001b[0m\u001b[1mi\u001b[22mpe\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1mu\u001b[22mffer \u001b[0m\u001b[1mi\u001b[22ms_\u001b[0m\u001b[1mb\u001b[22msd\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "ibu(data, train, x, y[, bins_y; kwargs...])\n",
       "```\n",
       "\n",
       "Iterative Bayesian Unfolding of the target distribution in the DataFrame `data`. The deconvolution is inferred from the DataFrame `train`, where the target column `y` and the observable column `x` are given.\n",
       "\n",
       "This function wraps `ibu(R, g; kwargs...)`, constructing `R` and `g` from the examples in the two DataFrames.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "ibu(x_data, x_train, y_train[, bins_y; kwargs...])\n",
       "```\n",
       "\n",
       "Iterative Bayesian Unfolding of the target distribution, given the observations in the one-dimensional array `x_data`.\n",
       "\n",
       "The deconvolution is inferred from `x_train` and `y_train`. Both of these arrays have to be discrete, i.e., they must contain indices instead of actual values. All expected label indices (for cases where `y_train` may not contain some of the indices) are optionally provided as `bins_y`.\n",
       "\n",
       "This function wraps `ibu(R, g; kwargs...)`, constructing `R` and `g` from the examples in the three arrays.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "ibu(R, g; kwargs...)\n",
       "```\n",
       "\n",
       "Iterative Bayesian Unfolding with the detector response matrix `R` and the observable density function `g`.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `f_0 = ones(m) ./ m` defines the prior, which is uniform by default.\n",
       "  * `smoothing = Base.identity` is a function that optionally applies smoothing in between iterations. The operation is neither applied to the initial prior, nor to the final result. The function `inspect` is called before the smoothing is performed.\n",
       "  * `K = 3` is the maximum number of iterations.\n",
       "  * `epsilon = 0.0` is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "  * `inspect = nothing` is a function `(f_k::Array, k::Int, chi2s::Float64) -> Any` optionally called in every iteration.\n"
      ],
      "text/plain": [
       "\u001b[36m  ibu(data, train, x, y[, bins_y; kwargs...])\u001b[39m\n",
       "\n",
       "  Iterative Bayesian Unfolding of the target distribution in the DataFrame\n",
       "  \u001b[36mdata\u001b[39m. The deconvolution is inferred from the DataFrame \u001b[36mtrain\u001b[39m, where the\n",
       "  target column \u001b[36my\u001b[39m and the observable column \u001b[36mx\u001b[39m are given.\n",
       "\n",
       "  This function wraps \u001b[36mibu(R, g; kwargs...)\u001b[39m, constructing \u001b[36mR\u001b[39m and \u001b[36mg\u001b[39m from the\n",
       "  examples in the two DataFrames.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  ibu(x_data, x_train, y_train[, bins_y; kwargs...])\u001b[39m\n",
       "\n",
       "  Iterative Bayesian Unfolding of the target distribution, given the\n",
       "  observations in the one-dimensional array \u001b[36mx_data\u001b[39m.\n",
       "\n",
       "  The deconvolution is inferred from \u001b[36mx_train\u001b[39m and \u001b[36my_train\u001b[39m. Both of these arrays\n",
       "  have to be discrete, i.e., they must contain indices instead of actual\n",
       "  values. All expected label indices (for cases where \u001b[36my_train\u001b[39m may not contain\n",
       "  some of the indices) are optionally provided as \u001b[36mbins_y\u001b[39m.\n",
       "\n",
       "  This function wraps \u001b[36mibu(R, g; kwargs...)\u001b[39m, constructing \u001b[36mR\u001b[39m and \u001b[36mg\u001b[39m from the\n",
       "  examples in the three arrays.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  ibu(R, g; kwargs...)\u001b[39m\n",
       "\n",
       "  Iterative Bayesian Unfolding with the detector response matrix \u001b[36mR\u001b[39m and the\n",
       "  observable density function \u001b[36mg\u001b[39m.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36mf_0 = ones(m) ./ m\u001b[39m defines the prior, which is uniform by default.\n",
       "\n",
       "    •    \u001b[36msmoothing = Base.identity\u001b[39m is a function that optionally applies\n",
       "        smoothing in between iterations. The operation is neither applied\n",
       "        to the initial prior, nor to the final result. The function\n",
       "        \u001b[36minspect\u001b[39m is called before the smoothing is performed.\n",
       "\n",
       "    •    \u001b[36mK = 3\u001b[39m is the maximum number of iterations.\n",
       "\n",
       "    •    \u001b[36mepsilon = 0.0\u001b[39m is the minimum symmetric Chi Square distance between\n",
       "        iterations. If the actual distance is below this threshold,\n",
       "        convergence is assumed and the algorithm stops.\n",
       "\n",
       "    •    \u001b[36minspect = nothing\u001b[39m is a function \u001b[36m(f_k::Array, k::Int,\n",
       "        chi2s::Float64) -> Any\u001b[39m optionally called in every iteration."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ibu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
