{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconvolution with DSEA\n",
    "\n",
    "The Dortmund Spectrum Estimation Algorithm (DSEA) reconstructs the target distribution from classifier predictions on the target quantity of individual examples. CherenkovDeconvolution.jl implements the improved version DSEA+, which is extended by adaptive step sizes and a fixed reweighting of examples.\n",
    "\n",
    "For a quick start, we deconvolve the distribution of Iris plant types in the famous IRIS data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the example data\n",
    "using MLDataUtils\n",
    "X, y_labels, _ = load_iris()\n",
    "\n",
    "# discretize the target quantity (for numerical values, we'd use LinearDiscretizer)\n",
    "using Discretizers: encode, CategoricalDiscretizer\n",
    "y = encode(CategoricalDiscretizer(y_labels), y_labels) # vector of target value indices\n",
    "\n",
    "# have a look at the content of y\n",
    "unique(y) # its just indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into training and observed data sets.\n",
    "# \n",
    "# The matrices MLDataUtils expects are transposed, by default.\n",
    "# Thus, we have to be explicit about obsdim = 1. Note that\n",
    "# CherenkovDeconvolution.jl follows the convention of ScikitLearn.jl\n",
    "# (and others), which is size(X_train) == (n_examples, n_features).\n",
    "# \n",
    "# MLDataUtils unfortunately assumes size(X_train) == (n_features, n_examples),\n",
    "# but obsdim = 1 fixes this assumption.\n",
    "# \n",
    "(X_train, y_train), (X_data, y_data) = splitobs(shuffleobs((X', y), obsdim = 1), obsdim = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mUtilities of ScikitLearn.jl are available in CherenkovDeconvolution.Sklearn\n",
      "\u001b[39m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.355556\n",
       " 0.357834\n",
       " 0.28661 "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Now let's estimate the target distribution!\n",
    "#\n",
    "using ScikitLearn, CherenkovDeconvolution\n",
    "\n",
    "# deconvolve with a Naive Bayes classifier\n",
    "@sk_import naive_bayes : GaussianNB\n",
    "tp_function = Sklearn.train_and_predict_proba(GaussianNB())\n",
    "\n",
    "f_est = dsea(X_data, X_train, y_train, tp_function) # returns a vector of target value probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.355556\n",
       " 0.311111\n",
       " 0.333333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Compare the result to the true target distribution, which we are estimating\n",
    "#\n",
    "f_true = Util.fit_pdf(y_data) # f_est is almost equal to f_true!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[1md\u001b[22m\u001b[1ms\u001b[22m\u001b[1me\u001b[22m\u001b[1ma\u001b[22m Gri\u001b[1md\u001b[22m\u001b[1mS\u001b[22m\u001b[1me\u001b[22m\u001b[1ma\u001b[22mrch \u001b[1mD\u001b[22men\u001b[1ms\u001b[22m\u001b[1me\u001b[22m\u001b[1mA\u001b[22mrray \u001b[1md\u001b[22me\u001b[1ms\u001b[22m\u001b[1me\u001b[22mri\u001b[1ma\u001b[22mlize \u001b[1mD\u001b[22men\u001b[1ms\u001b[22m\u001b[1me\u001b[22mM\u001b[1ma\u001b[22mtrix \u001b[1mD\u001b[22men\u001b[1ms\u001b[22m\u001b[1me\u001b[22mVecOrM\u001b[1ma\u001b[22mt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "dsea(data, train, y, train_and_predict_proba;\n",
       "     features = setdiff(names(train), [y]),\n",
       "     kwargs...)\n",
       "```\n",
       "\n",
       "Deconvolve the `y` distribution in the DataFrame `data`, as learned from the DataFrame `train`. This function wraps `dsea(::Matrix, ::Matrix, ::Array, ::Function)`.\n",
       "\n",
       "The additional keyword arguments allows to specify the columns in `data` and `train` to be used as the `features`.\n",
       "\n",
       "```\n",
       "dsea(X_data, X_train, y_train, train_and_predict_proba; kwargs...)\n",
       "```\n",
       "\n",
       "Deconvolve the target distribution of `X_data`, as learned from `X_train` and `y_train`. The function `train_and_predict_proba` trains and applies a classifier. It has the signature `(X_data, X_train, y_train, w_train) -> Any` where all arguments but `w_train`, which is updated in each iteration, are simply passed through. To facilitate classification, `y_train` has to be discrete, i.e., it has to have a limited number of unique values that are used as labels for the classifier.\n",
       "\n",
       "# Keyword arguments\n",
       "\n",
       "  * `f_0 = ones(m) ./ m` defines the prior, which is uniform by default\n",
       "  * `fixweighting = false` sets, whether or not the weight update fix is applied. This fix is proposed in my Master's thesis and in the corresponding paper.\n",
       "  * `alpha = 1.0` is the step size taken in every iteration. This parameter can be either a constant value or a function with the signature `(k::Int, pk::AbstractArray{Float64,1}, f_prev::AbstractArray{Float64,1} -> Float`, where `f_prev` is the estimate of the previous iteration and `pk` is the direction that DSEA takes in the current iteration `k`.\n",
       "  * `smoothing = Base.identity` is a function that optionally applies smoothing in between iterations\n",
       "  * `K = 1` is the maximum number of iterations.\n",
       "  * `epsilon = 0.0` is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "  * `inspect = nothing` is a function `(k::Int, alpha::Float64, chi2s::Float64, spectrum::Array) -> Any` optionally called in every iteration.\n",
       "  * `loggingstream = DevNull` is an optional `IO` stream to write log messages to.\n",
       "  * `return_contributions = false` sets, whether or not the contributions of individual examples in `X_data` are returned as a tuple together with the deconvolution result.\n"
      ],
      "text/plain": [
       "```\n",
       "dsea(data, train, y, train_and_predict_proba;\n",
       "     features = setdiff(names(train), [y]),\n",
       "     kwargs...)\n",
       "```\n",
       "\n",
       "Deconvolve the `y` distribution in the DataFrame `data`, as learned from the DataFrame `train`. This function wraps `dsea(::Matrix, ::Matrix, ::Array, ::Function)`.\n",
       "\n",
       "The additional keyword arguments allows to specify the columns in `data` and `train` to be used as the `features`.\n",
       "\n",
       "```\n",
       "dsea(X_data, X_train, y_train, train_and_predict_proba; kwargs...)\n",
       "```\n",
       "\n",
       "Deconvolve the target distribution of `X_data`, as learned from `X_train` and `y_train`. The function `train_and_predict_proba` trains and applies a classifier. It has the signature `(X_data, X_train, y_train, w_train) -> Any` where all arguments but `w_train`, which is updated in each iteration, are simply passed through. To facilitate classification, `y_train` has to be discrete, i.e., it has to have a limited number of unique values that are used as labels for the classifier.\n",
       "\n",
       "# Keyword arguments\n",
       "\n",
       "  * `f_0 = ones(m) ./ m` defines the prior, which is uniform by default\n",
       "  * `fixweighting = false` sets, whether or not the weight update fix is applied. This fix is proposed in my Master's thesis and in the corresponding paper.\n",
       "  * `alpha = 1.0` is the step size taken in every iteration. This parameter can be either a constant value or a function with the signature `(k::Int, pk::AbstractArray{Float64,1}, f_prev::AbstractArray{Float64,1} -> Float`, where `f_prev` is the estimate of the previous iteration and `pk` is the direction that DSEA takes in the current iteration `k`.\n",
       "  * `smoothing = Base.identity` is a function that optionally applies smoothing in between iterations\n",
       "  * `K = 1` is the maximum number of iterations.\n",
       "  * `epsilon = 0.0` is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "  * `inspect = nothing` is a function `(k::Int, alpha::Float64, chi2s::Float64, spectrum::Array) -> Any` optionally called in every iteration.\n",
       "  * `loggingstream = DevNull` is an optional `IO` stream to write log messages to.\n",
       "  * `return_contributions = false` sets, whether or not the contributions of individual examples in `X_data` are returned as a tuple together with the deconvolution result.\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?dsea # You can find more information in the documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
